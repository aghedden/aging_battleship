{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f35f30-5105-472a-9931-fb5dfc6521d8",
   "metadata": {},
   "source": [
    "# Random and Directed Exploration Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfb832-e6c4-48b4-9ca6-53d5ae094a74",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e210d-db7e-41a6-a451-5be3c7d336f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n",
    "from scipy.special import gammaln, digamma\n",
    "from scipy.stats import dirichlet\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29df7fc-6dca-4c64-9318-1b7941fe27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make age dictionary\n",
    "subject_ids = ['YA01', 'YA02', 'YA03', 'YA04', 'YA05', 'YA06', 'YA07', 'YA08', 'YA09', 'YA10', 'YA11', 'YA12', 'YA13', 'YA14', 'YA15', 'YA16', 'YA17', 'YA18', 'YA19', 'YA20', 'YA21', 'YA22', 'YA23', 'YA24', 'YA25', 'YA26', 'YA27', 'YA28', 'YA29', 'YA30', 'YA31', 'YA32', 'YA33', 'YA34', 'YA35', 'YA36', 'YA37', 'YA38', 'YA39', 'YA40', 'YA41', 'YA42', 'YA43', 'YA44', 'YA45', 'YA46', 'YA47', 'YA48', 'YA49', 'YA50', 'YA51', 'YA52', 'YA53', 'YA54', 'YA55', 'YA56', 'YA57', 'MA01', 'MA02', 'MA03', 'MA04', 'MA05', 'MA06', 'MA07', 'MA08', 'MA09', 'MA10', 'MA11', 'MA12', 'MA13', 'MA14', 'MA15', 'MA16', 'MA17', 'MA18', 'MA19', 'MA20', 'MA21', 'MA22', 'MA23', 'MA24', 'MA25', 'MA26', 'MA27', 'MA28', 'MA29', 'MA30', 'MA31', 'MA32', 'MA33', 'MA34', 'MA35', 'MA36', 'MA37', 'MA38', 'MA39', 'MA40', 'MA41', 'MA42', 'MA43', 'MA44', 'MA45', 'MA46', 'MA47', 'MA48', 'MA49', 'MA50', 'MA51', 'MA52', 'MA53', 'MA54', 'MA55', 'MA56', 'MA57', 'MA58', 'MA59', 'OA01', 'OA02', 'OA03', 'OA04', 'OA05', 'OA06', 'OA07', 'OA08', 'OA09', 'OA10', 'OA11', 'OA12', 'OA13', 'OA14', 'OA15', 'OA16', 'OA17', 'OA18', 'OA19', 'OA20', 'OA21', 'OA22', 'OA23', 'OA24', 'OA25', 'OA26', 'OA27', 'OA28', 'OA29', 'OA30', 'OA31', 'OA32', 'OA33', 'OA34', 'OA35', 'OA36', 'OA37', 'OA38', 'OA39', 'OA40', 'OA41', 'OA42', 'OA43', 'OA44', 'OA45', 'OA46', 'OA47', 'OA48', 'OA49', 'OA50', 'OA51', 'OA52', 'OA53']\n",
    "ages = [20, 19, 19, 18, 20, 22, 21, 20, 19, 19, 20, 19, 20, 24, 20, 19, 21, 19, 21, 19, 20, 22, 20, 19, 20, 26, 19, 19, 18, 20, 20, 19, 21, 20, 18, 19, 20, 20, 22, 20, 20, 21, 22, 19, 19, 21, 19, 19, 20, 20, 20, 20, 21, 20, 20, 21, 21, 44, 42, 42, 48, 35, 45, 39, 39, 39, 37, 40, 38, 38, 40, 39, 39, 43, 37, 46, 36, 39, 42, 45, 38, 35, 38, 38, 47, 37, 40, 41, 43, 48, 40, 41, 43, 36, 43, 43, 48, 43, 42, 38, 38, 48, 43, 38, 37, 40, 43, 36, 47, 44, 46, 39, 40, 45, 37, 39, 54, 64, 51, 53, 71, 58, 55, 52, 52, 57, 51, 55, 52, 60, 62, 56, 56, 60, 57, 59, 63, 64, 61, 57, 59, 56, 56, 56, 59, 52, 56, 71, 55, 64, 50, 54, 56, 53, 65, 70, 54, 51, 66, 54, 50, 51, 57, 53, 56, 51, 52, 57, 51]\n",
    "\n",
    "# create a dictionary mapping subject ids to ages\n",
    "subject_age_dict = dict(zip(subject_ids, ages))\n",
    "\n",
    "# add ages using dictionary\n",
    "# df['age'] = df['subject_id'].map(subject_age_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96277d3a-a327-4ffb-9578-7383c1891c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cpt dictionary\n",
    "subject_ids = ['YA01', 'YA02', 'YA03', 'YA04', 'YA05', 'YA06', 'YA07', 'YA08', 'YA09', 'YA10', 'YA11', 'YA12', 'YA13', 'YA14', 'YA15', 'YA16', 'YA17', 'YA18', 'YA19', 'YA20', 'YA21', 'YA22', 'YA23', 'YA24', 'YA25', 'YA26', 'YA27', 'YA28', 'YA29', 'YA30', 'YA31', 'YA32', 'YA33', 'YA34', 'YA35', 'YA36', 'YA37', 'YA38', 'YA39', 'YA40', 'YA41', 'YA42', 'YA43', 'YA44', 'YA45', 'YA46', 'YA47', 'YA48', 'YA49', 'YA50', 'YA51', 'YA52', 'YA53', 'YA54', 'YA55', 'YA56', 'YA57', 'MA01', 'MA02', 'MA03', 'MA04', 'MA05', 'MA06', 'MA07', 'MA08', 'MA09', 'MA10', 'MA11', 'MA12', 'MA13', 'MA14', 'MA15', 'MA16', 'MA17', 'MA18', 'MA19', 'MA20', 'MA21', 'MA22', 'MA23', 'MA24', 'MA25', 'MA26', 'MA27', 'MA28', 'MA29', 'MA30', 'MA31', 'MA32', 'MA33', 'MA34', 'MA35', 'MA36', 'MA37', 'MA38', 'MA39', 'MA40', 'MA41', 'MA42', 'MA43', 'MA44', 'MA45', 'MA46', 'MA47', 'MA48', 'MA49', 'MA50', 'MA51', 'MA52', 'MA53', 'MA54', 'MA55', 'MA56', 'MA57', 'MA58', 'MA59', 'OA01', 'OA02', 'OA03', 'OA04', 'OA05', 'OA06', 'OA07', 'OA08', 'OA09', 'OA10', 'OA11', 'OA12', 'OA13', 'OA14', 'OA15', 'OA16', 'OA17', 'OA18', 'OA19', 'OA20', 'OA21', 'OA22', 'OA23', 'OA24', 'OA25', 'OA26', 'OA27', 'OA28', 'OA29', 'OA30', 'OA31', 'OA32', 'OA33', 'OA34', 'OA35', 'OA36', 'OA37', 'OA38', 'OA39', 'OA40', 'OA41', 'OA42', 'OA43', 'OA44', 'OA45', 'OA46', 'OA47', 'OA48', 'OA49', 'OA50', 'OA51', 'OA52', 'OA53']\n",
    "\n",
    "cpts = [84, 33, 49, 67, 39, 66, 36, 68, 64, 11, 58, 36, 81, 24, 31, 26, 77, 0, 65, 84, 49, 24, 21, 51, 72, 12, 69, 60, 58, 24, 51, 0, 76, 22, 58, 36, 72, 52, 59, 53, 55, 22, 62, 30, 44, 51, 42, 9, 0, 66, 27, 67, 38, 40, 3, 49, 33, 51, 80, 29, 13, 43, 83, 35, 58, 55, 0, 31, 43, 33, 33, 41, 37, 11, 57, 34, 56, 63, 12, 13, 35, 60, 0, 58, 15, 11, 31, 33, 36, 31, 64, 29, 36, 46, 36, 55, 39, 0, 37, 8, 77, 0, 46, 55, 22, 67, 29, 53, 30, 16, 28, 38, 23, 33, 74, 50, 82, 26, 25, 64, 52, 26, 40, 0, 2, 17, 0, 60, 53, 25, 5, 44, 16, 50, 15, 24, 13, 0, 21, 32, 23, 16, 59, 48, 0, 63, 56, 45, 0, 62, 7, 53, 70, 57, 31, 0, 15, 30, 30, 44, 51, 68, 48, 73, 44, 76, 67, 35, 36]\n",
    "\n",
    "# create a dictionary mapping subject ids to cpts\n",
    "subject_cpt_dict = dict(zip(subject_ids, cpts))\n",
    "\n",
    "# add cpts using dictionary\n",
    "# df['cpt'] = df['subject_id'].map(subject_cpt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1a00b-9961-4407-8ec8-1a46b4a5ad41",
   "metadata": {},
   "source": [
    "## Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0f28d-0b02-4173-90c1-a85e32bf25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/data/all_raw_data/event_totals_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74905064-eb78-4d18-bcc9-e21fddf85163",
   "metadata": {},
   "source": [
    "### *Load choices_wide_df below to skip this first section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac1244-76f0-4d0b-b871-089e9d6f2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_wide_df=pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/analysis/choices_wide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc282fe5-bc14-43b6-87be-93c06e8afe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create master grid of all tile coordinates\n",
    "tile_coords = np.array(list(itertools.product(range(5), range(5))))  # shape (25, 2)\n",
    "\n",
    "# empty list to store data\n",
    "rows = []\n",
    "\n",
    "# loop through each subject\n",
    "for s in df.subject_id.unique():\n",
    "    # make a tmp df for that subject's 'events' data\n",
    "    tmp = pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/data/all_raw_data/' + s + '_events.csv')\n",
    "\n",
    "    # fill forward choice_num (for MMS/MHS values, NaN) (because of the choice location rows being empty for some reason)\n",
    "    tmp['choice_num'] = tmp['choice_num'].ffill()\n",
    "    tmp['choice_num'] = tmp['choice_num'].astype(int)\n",
    "\n",
    "    # keep only MMS/MHS rows (choice outcomes)\n",
    "    tmp = tmp[tmp['x'].isin(['MMS', 'MHS'])]\n",
    "\n",
    "    for trial in tmp['trial_num'].unique():\n",
    "        trial_data = tmp[tmp['trial_num'] == trial].copy()\n",
    "        trial_data.sort_values('choice_num', inplace=True)\n",
    "\n",
    "        tiles_already_chosen = set() # create a set, like a list but with no duplicates, that stores the coords of every tile chosen so far in this trial\n",
    "        # row and col of the tile chosen in the previous choice. set to nan initially because there is no previous choice for the first choice\n",
    "        prev_tile_row = np.nan\n",
    "        prev_tile_col = np.nan\n",
    "\n",
    "        # go through each row of the trial_data DataFrame one by one\n",
    "        # pull the choice data\n",
    "        for _, row_data in trial_data.iterrows():\n",
    "            choice_num = row_data['choice_num']\n",
    "            chosen_row = int(row_data['y'])\n",
    "            chosen_col = int(row_data['id'])\n",
    "            choice_outcome = row_data['x']\n",
    "\n",
    "            # compute distances from previous tile to ALL tiles\n",
    "            #if pd.isna(prev_tile_row): # check if this is the first choice (prev_tile_row is nan)\n",
    "            if choice_num == 1: # if this is the first choice in the trial\n",
    "                distances = np.full(len(tile_coords), np.nan) # if it is the first choice -> create an array of 25 nan values (one for each possible tile)\n",
    "            else: # if not the first choice, calculate the  distance from the previous tile to every possible tile\n",
    "                distances = np.linalg.norm(\n",
    "                    tile_coords - np.array([prev_tile_col, prev_tile_row]), # creates a coordinate pair for the previous tile and subtract it from every tile on the grid\n",
    "                    axis=1\n",
    "                )\n",
    "                                                   \n",
    "            # mask out already chosen tiles\n",
    "            # creates a boolean array by checking each coordinate in tile_coords to see if it's already been chosen in this trial\n",
    "            mask = np.array([\n",
    "                (x, y) in tiles_already_chosen\n",
    "                for x, y in tile_coords\n",
    "            ]) # creates a list of 25 true/false values\n",
    "            distances[mask] = np.nan # sets the distance to nan for any tiles that have already been chosen\n",
    "\n",
    "            # create dict for the row\n",
    "            row_dict = {\n",
    "                'subject_id': s,\n",
    "                'trial_num': trial,\n",
    "                'choice_num': choice_num,\n",
    "                'chosen_tile_col': chosen_col,\n",
    "                'chosen_tile_row': chosen_row,\n",
    "                'prev_tile_col': prev_tile_col,\n",
    "                'prev_tile_row': prev_tile_row,\n",
    "                'choice_outcome': choice_outcome\n",
    "            }\n",
    "\n",
    "            # add distances to the dict\n",
    "            # create a distance column for every tile on the grid from (0,0) to (4,4)\n",
    "            for i, (x, y) in enumerate(tile_coords):\n",
    "                col_name = f'dist_tile_{x}_{y}'\n",
    "                row_dict[col_name] = distances[i]\n",
    "\n",
    "            rows.append(row_dict)\n",
    "\n",
    "            # update chosen tiles and previous tile\n",
    "            tiles_already_chosen.add((chosen_col, chosen_row))\n",
    "            prev_tile_row = chosen_row\n",
    "            prev_tile_col = chosen_col\n",
    "\n",
    "# convert all rows to df\n",
    "choices_wide_df = pd.DataFrame(rows)\n",
    "\n",
    "# add dist_chosen column\n",
    "# construct the column names for each chosen tile\n",
    "chosen_col_names = (\n",
    "    \"dist_tile_\" +\n",
    "    choices_wide_df[\"chosen_tile_col\"].astype(str) + \"_\" +\n",
    "    choices_wide_df[\"chosen_tile_row\"].astype(str)\n",
    ")\n",
    "\n",
    "# retreive the distance value for each chosen tile\n",
    "choices_wide_df[\"dist_chosen\"] = [\n",
    "    choices_wide_df.loc[i, col] if col in choices_wide_df.columns else np.nan\n",
    "    for i, col in enumerate(chosen_col_names)\n",
    "]\n",
    "\n",
    "# add age and cpt\n",
    "choices_wide_df[\"age\"] = choices_wide_df[\"subject_id\"].map(subject_age_dict)\n",
    "choices_wide_df[\"cpt\"] = choices_wide_df[\"subject_id\"].map(subject_cpt_dict)\n",
    "\n",
    "choices_wide_df.to_csv(\"choices_wide.csv\", index=False)\n",
    "\n",
    "print(choices_wide_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a52203-c044-4aa0-baae-7e8c08951d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy to play around with\n",
    "exp_choices_df = choices_wide_df.copy()\n",
    "exp_choices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa507e4-4cc3-417b-9f2b-fa391d9bc4ad",
   "metadata": {},
   "source": [
    "## Calculate Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a1152-4838-4d4d-8a6c-7b32f74b58ad",
   "metadata": {},
   "source": [
    "### Expected Reward\n",
    "- In the first trial, all expected rewards = 0 because there is no history yet \n",
    "- In all following trials, expected reward = cumulative hits/cumulative choices from *previous trials only* (aka it doesn't include the current choice's outcome in the reward calculation... I was also not doing this before)\n",
    "- If a tile has never been chosen before, expected reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810013f-14e3-4efa-89b5-b064b1e4272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tile coordinate labels\n",
    "tile_coords = [(i, j) for i in range(5) for j in range(5)]\n",
    "\n",
    "# create reward columns\n",
    "for x, y in tile_coords:\n",
    "    exp_choices_df[f\"rew_tile_{x}_{y}\"] = np.nan\n",
    "# create rew_chosen column in main df, set to nan for now\n",
    "\n",
    "exp_choices_df[\"rew_chosen\"] = np.nan\n",
    "# sort to preserve trial order\n",
    "exp_choices_df.sort_values([\"subject_id\", \"trial_num\", \"choice_num\"], inplace=True)\n",
    "\n",
    "for subject_id, subj_df in exp_choices_df.groupby(\"subject_id\", sort=False):\n",
    "    subj_df = subj_df.copy()\n",
    "    subj_df = subj_df.sort_values([\"trial_num\", \"choice_num\"])\n",
    "    \n",
    "    # initialize tracking dictionaries for this subject\n",
    "    tile_hits = {}  # tile_pos: total_hits_so_far\n",
    "    tile_counts = {}  # tile_pos: total_choices_so_far\n",
    "    \n",
    "    # process each choice in each trail\n",
    "    for trial_num in subj_df['trial_num'].unique():\n",
    "        trial_df = subj_df[subj_df['trial_num'] == trial_num].copy()\n",
    "        \n",
    "        # at the START of trial, calculate expected rewards for ALL tiles based on history... this is the expected info for that tile this round\n",
    "        trial_tile_rewards = {}\n",
    "        for x, y in tile_coords:\n",
    "            tile_pos = (x, y)\n",
    "            if tile_pos in tile_counts and tile_counts[tile_pos] > 0:\n",
    "                trial_tile_rewards[f\"rew_tile_{x}_{y}\"] = tile_hits[tile_pos] / tile_counts[tile_pos]\n",
    "            else:\n",
    "                trial_tile_rewards[f\"rew_tile_{x}_{y}\"] = 0.0\n",
    "        \n",
    "        # apply these same reward values to ALL choices in this trial\n",
    "        for idx, row in trial_df.iterrows():\n",
    "            # set expected reward for chosen tile\n",
    "            chosen_tile_pos = (int(row['chosen_tile_col']), int(row['chosen_tile_row']))\n",
    "            if chosen_tile_pos in tile_counts and tile_counts[chosen_tile_pos] > 0:\n",
    "                subj_df.loc[idx, \"rew_chosen\"] = tile_hits[chosen_tile_pos] / tile_counts[chosen_tile_pos]\n",
    "            else:\n",
    "                subj_df.loc[idx, \"rew_chosen\"] = 0.0\n",
    "            \n",
    "            # set reward values for all tiles (same values for entire trial)\n",
    "            for col_name, reward_val in trial_tile_rewards.items():\n",
    "                subj_df.loc[idx, col_name] = reward_val\n",
    "        \n",
    "        # AFTER processing the entire trial, update counts with this trial's outcomes\n",
    "        for idx, row in trial_df.iterrows():\n",
    "            tile_pos = (int(row['chosen_tile_col']), int(row['chosen_tile_row']))\n",
    "            \n",
    "            if tile_pos not in tile_hits:\n",
    "                tile_hits[tile_pos] = 0\n",
    "                tile_counts[tile_pos] = 0\n",
    "                \n",
    "            is_hit = 1 if row['choice_outcome'] == 'MHS' else 0\n",
    "            tile_hits[tile_pos] += is_hit\n",
    "            tile_counts[tile_pos] += 1\n",
    "    \n",
    "    # update the main dataframe\n",
    "    reward_cols = [f\"rew_tile_{x}_{y}\" for x, y in tile_coords]\n",
    "    exp_choices_df.loc[subj_df.index, reward_cols + [\"rew_chosen\"]] = subj_df[reward_cols + [\"rew_chosen\"]].values\n",
    "\n",
    "exp_choices_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e451297-8430-4180-bfca-3a638923f51c",
   "metadata": {},
   "source": [
    "### Historical Reward\n",
    "This is just calculating the number of times rewarded/number of times chosen. Calculates the current expected reward for each tile at each choice point. did not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5b7f8-e5e7-4923-a79e-13ae9f64c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define tile coordinate labels\n",
    "# tile_coords = [(i, j) for i in range(5) for j in range(5)]\n",
    "\n",
    "# # create reward columns\n",
    "# for x, y in tile_coords:\n",
    "#     exp_choices_df[f\"rew_tile_{x}_{y}\"] = np.nan\n",
    "\n",
    "# # create rew_chosen column in main df, set to nan for now\n",
    "# exp_choices_df[\"rew_chosen\"] = np.nan\n",
    "\n",
    "# # sort to preserve trial order\n",
    "# exp_choices_df.sort_values([\"subject_id\", \"trial_num\", \"choice_num\"], inplace=True)\n",
    "\n",
    "# for subject_id, subj_df in exp_choices_df.groupby(\"subject_id\", sort=False): # for each set of subject_id data\n",
    "#     subj_df = subj_df.copy() # for safe modification\n",
    "\n",
    "#     # make an index that we can merge on later\n",
    "#     subj_df[\"row_idx\"] = subj_df.index\n",
    "\n",
    "#     # extract chosen tiles and outcome information (this is what we need for our reward calcs)\n",
    "#     chosen_tiles = pd.DataFrame({\n",
    "#         \"row_idx\": subj_df.index,\n",
    "#         \"tile_row\": subj_df[\"chosen_tile_row\"],\n",
    "#         \"tile_col\": subj_df[\"chosen_tile_col\"],\n",
    "#         \"is_hit\": (subj_df[\"choice_outcome\"] == \"MHS\").astype(int)\n",
    "#     })\n",
    "\n",
    "#     # group by tile to get cumulative counts\n",
    "#     chosen_tiles[\"tile_pos\"] = list(zip(chosen_tiles[\"tile_col\"], chosen_tiles[\"tile_row\"]))\n",
    "#     chosen_tiles[\"cum_total\"] = (chosen_tiles.groupby(\"tile_pos\").cumcount() + 1) # tracks how many times each tile has been chosen\n",
    "#     chosen_tiles[\"cum_hits\"] = (chosen_tiles.groupby(\"tile_pos\")[\"is_hit\"].cumsum()) # tracks how many times each tile has been rewarded in the past\n",
    "#     chosen_tiles[\"reward\"] = (chosen_tiles[\"cum_hits\"]/chosen_tiles[\"cum_total\"]) # calculates reward based on times chosen and rewarded in past for tile\n",
    "\n",
    "#     # attach rewards for chosen tiles\n",
    "#     subj_df[\"rew_chosen\"] = chosen_tiles[\"reward\"].values\n",
    "\n",
    "#     # assign the rewards into the appropriate rew_tile_X_Y column\n",
    "#     for (x, y), grp in chosen_tiles.groupby(\"tile_pos\"):\n",
    "#         rew_col = f\"rew_tile_{x}_{y}\"\n",
    "#         subj_df.loc[grp[\"row_idx\"], rew_col] = grp[\"reward\"].values\n",
    "\n",
    "#     # forward-fill rewards across rows for unchosen tile (reward value not updating for non-chosen tiles)\n",
    "#     subj_df = subj_df.sort_values([\"trial_num\", \"choice_num\"])\n",
    "#     reward_cols = [f\"rew_tile_{x}_{y}\" for x, y in tile_coords]\n",
    "#     subj_df[reward_cols] = subj_df[reward_cols].ffill().fillna(0.0)\n",
    "\n",
    "#     # update the main df\n",
    "#     exp_choices_df.loc[subj_df.index, reward_cols + [\"rew_chosen\"]] = subj_df[reward_cols + [\"rew_chosen\"]]\n",
    "\n",
    "# exp_choices_df.sort_index(inplace=True)\n",
    "\n",
    "# #exp_choices_df.to_csv(\"exp_choices_df.csv\", index=False)\n",
    "\n",
    "# # preview result\n",
    "# print(exp_choices_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84026421-ec2f-48f9-adab-f6829b3de048",
   "metadata": {},
   "source": [
    "## Calculate Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c41407-6930-4ed2-a3a2-87a09974f34d",
   "metadata": {},
   "source": [
    "### Information notes:\n",
    "Dirichlet Distribution notes:\n",
    "- https://en.wikipedia.org/wiki/Dirichlet_process\n",
    "- https://en.wikipedia.org/wiki/Dirichlet_distribution\n",
    "-  The base distribution is the expected value of the process, i.e., the Dirichlet process draws distributions \"around\" the base distribution the way a normal distribution draws real numbers around its mean.\n",
    "\n",
    "Information itself is the difference in Shannon entropy before and after each choice. The infos from MATLAB used in the analyses are the **EXPECTED INFOs**\n",
    "<br>\n",
    "\n",
    "**Expected info**\n",
    "- The expected information calculation is much more complex - it requires computing the expected entropy reduction for each potential choice before the choice is made, not after\n",
    "\n",
    "Implement expected information calculation for each tile at each choice point:\n",
    "- For each possible choice at each decision point\n",
    "- Calculate the expected information gain if that tile were chosen\n",
    "- This requires computing expected entropy reduction across all possible outcomes\n",
    "\n",
    "- For each remaining tile (remaining_tiles): Calculate expected information if that tile were chosen\n",
    "- For each possible outcome (o=0:1): Simulate both MISS (0) and HIT (1)\n",
    "- Create likelihood vector: li_next(o==data(s).patterns(:,remaining_tiles(k)))=1\n",
    "- Update probabilities: Multiply current beliefs by likelihood and normalize\n",
    "- Calculate entropy after each simulated outcome\n",
    "- Expected information: EI(k) = H_S - mean(H_next) (current entropy minus average of post-outcome entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a4004-81d9-4f96-9c89-42f0631dbc3e",
   "metadata": {},
   "source": [
    "### Initiate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ff3ff-0a99-45a9-b617-5a4cd9bfda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the initial board configuration\n",
    "grid_size = (5, 5)\n",
    "shape_grid_size = (3, 3)  # grid that the shapes fit into\n",
    "min_shape_size = 3\n",
    "max_shape_size = 9\n",
    "initial_counts = 2.0  # initial alpha values for Dirichlet\n",
    "# \"2 was chosen as the simplest, smallest, model-free numerically stable initial count\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87600cd4-b9ad-45d0-9a29-7ce2329fb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize arrays to track tile choices for expected information calculation\n",
    "tile_information_sums = np.zeros(grid_size)\n",
    "tile_choice_counts = np.zeros(grid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303c273-65d1-4ad9-8570-0af9acfb0432",
   "metadata": {},
   "source": [
    "### Possible states\n",
    "#### `generate_all_possible_states`: generate all possible states following rules of shapes (3-8 tiles, connected by side)\n",
    "#### `generate_states_like_matlab`: generate all possible states mimicking Dave's MATLAB code exactly\n",
    "#### `is_connected`: check if tiles are connected; used in `generate_all_possible_states`\n",
    "\"These boards were formed by possible combinations of connected, filled tiles on a 3×3 grid that were then placed on the 5×5 board. The shapes had to be at least 3 tiles large and connections had to be in the vertical or horizontal directions (i.e., no diagonal-only connections permitted).\"\n",
    "\n",
    "MATLAB code:\n",
    "The MATLAB code generates all 512 possible 3×3 binary patterns, including ones with:\n",
    "- 0 tiles filled (all zeros),\n",
    "- 1 tile filled,\n",
    "- 2 tiles filled,\n",
    "- up to all 9 tiles filled.\n",
    "\n",
    "It does not exclude shapes with fewer than 3 tiles, nor does it check connectivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8493d8-21e8-4012-956f-9f35e2d050e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''USED THIS ONE IN ANALYSES TO GET 3904 STATES!!!'''\n",
    "# mimicking Dave's code exactly\n",
    "# includes shapes with 0, 1, and 2 tiles, contrary to minimum of 3-tile requirement\n",
    "def generate_states_like_matlab():\n",
    "    # only run this if the board is 5x5\n",
    "    rows, cols = 5, 5\n",
    "    if rows == 5 and cols == 5:\n",
    "    \n",
    "        data = {}  # mimic MATLAB's data struct\n",
    "        set_idx = 0  # Just one 'set' in this simplified translation\n",
    "    \n",
    "        # generate all 2^9 = 512 binary combinations for 3x3 grid\n",
    "        all_patterns = list(itertools.product([0, 1], repeat=9))  # 512 total\n",
    "        all_patterns = np.array(all_patterns)\n",
    "    \n",
    "        # reshape each pattern into a 3x3 pattern on a 3x3 grid\n",
    "        patterns_3x3 = all_patterns.reshape((-1, 3, 3))  # shape: (512, 3, 3)\n",
    "    \n",
    "        # create the 9 places on the 5x5 grid where the 3x3 grid can be placed \n",
    "        embedded_patterns = []\n",
    "        positions = [(0,0), (0,1), (0,2), # top row\n",
    "                     (1,0), (1,1), (1,2), # middle row\n",
    "                     (2,0), (2,1), (2,2)] # bottom row\n",
    "    \n",
    "        for (r_off, c_off) in positions: # loop thorugh each possible placement location for the shape\n",
    "            for pat in patterns_3x3:\n",
    "                board = np.zeros((5, 5), dtype=int)\n",
    "                board[r_off:r_off+3, c_off:c_off+3] = pat\n",
    "                board = np.flipud(board)  \n",
    "                embedded_patterns.append(board.flatten())\n",
    "    \n",
    "        # remove duplicates\n",
    "        unique_patterns = np.unique(embedded_patterns, axis=0)\n",
    "\n",
    "        all_states = [pattern.reshape(5, 5) for pattern in unique_patterns]\n",
    "\n",
    "        # print first 5\n",
    "        print(\"\\nFirst 5 shapes:\")\n",
    "        for i, state in enumerate(all_states[:5]):\n",
    "            print(f\"\\nShape {i+1}:\\n{state}\")\n",
    "\n",
    "        # print last 5\n",
    "        print(\"\\nLast 5 shapes:\")\n",
    "        for i, state in enumerate(all_states[-5:]):\n",
    "            print(f\"\\nShape {len(all_states)-5 + i + 1}:\\n{state}\")\n",
    "\n",
    "        # return as list of 2D arrays\n",
    "        return all_states\n",
    "\n",
    "# call function\n",
    "all_states = generate_states_like_matlab()\n",
    "print(f\"\\nNumber of unique patterns (shapes, not locations on larger grid): {len(patterns)}\")\n",
    "for i, pattern in enumerate(patterns_3x3):\n",
    "    print(f\"Pattern {i + 1}:\")\n",
    "    for row in pattern:\n",
    "        print(row)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872c968-6d64-4d69-8ca0-478680ede532",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DID NOT USE!!! COMMENTING OUT FOR NOW BUT KEEPING AS REFERENCE IF NEEDED LATER'''\n",
    "# # FOLLOWING RULES OF PAPER (MIN TILES IN SHAPE IS 3 AND TILES HAVE TO BE CONNECTED BY EDGE, NOT JUST DIAGONAL\n",
    "# # define function generate all 3904 possible connected shapes that can appear on 5x5 grid\n",
    "# # creates all possible small shapes (patterns of 1s on the grid)\n",
    "# # need to create all possible patterns that are a certain size, are placed in a 3x3 grid, and are connected\n",
    "# # then embed each of those possible patterns/shapes into a 5x5 grid, in every possible position on the larger grid, so that the shape can appear anywhere on the 5x5 grid\n",
    "\n",
    "# def generate_all_possible_states():\n",
    "#     states = [] # initialize an empty list states to collect all valid 5×5 patterns\n",
    "#     '''NEED TO UPDATE 3x3 CODE BELOW FOR PD PARTICIPANTS SINCE ONE OF THE SHAPES IS OUTSIDE A 3x3 GRID!!!!'''\n",
    "#     # generate all possible 3x3 patterns (aka possible shapes)\n",
    "#     for n_filled in range(min_shape_size, min(max_shape_size+1, 10)): # loop over possible shape sizes; sets the range of shape sizes (aka how many tiles/1s in a shape); if min=3 and max=5, it loops through 3,4,and 5; the 10 at the end maes sure you don't try to make a shape with more than 9 tiles, since 9 is the max in the 3x3 grid\n",
    "#         for positions in itertools.combinations(range(9), n_filled): # generate all ways to choosen n_filled out of 9 to place 1s (part of shape); in other words, creates all unique ways the tiles in each sized shape can be laid out\n",
    "#             pattern = np.zeros((3, 3), dtype=int)\n",
    "#             for pos in positions:\n",
    "#                 row, col = pos//3, pos%3 #//3 gives row num and is how many times 3 goes into pos value, %3 give col num and is the remaineder\n",
    "#                                         # if pos = 5, then pos/3=1, and pos%3=2; so pos 5 is at row 1 and column 2; thsis just helps translate positions into a grid\n",
    "#                 pattern[row, col] = 1\n",
    "#                 # positions = (0, 3, 4) gives: [1 0 0]\n",
    "#                                              # [1 1 0]\n",
    "#                                              # [0 0 0]\n",
    "           \n",
    "#             # check if pattern is connected (flood fill) using function to make sure all 1s are connected\n",
    "#             if is_connected(pattern):\n",
    "#                 # place pattern on 5x5 grid at different positions\n",
    "#                 for start_row in range(grid_size[0]-2): # since the grid we are fitting inside is 3x3, need to only place in positions that won't make it go past larger 5x5 board grid\n",
    "#                     for start_col in range(grid_size[1]-2): # for each column and row in the 5x5 grid...\n",
    "#                         full_grid = np.zeros(grid_size, dtype=int) # create a blank 5x5 grid\n",
    "#                         full_grid[start_row:start_row+3, start_col:start_col+3] = pattern # place the 3x3 pattern in a subgrid\n",
    "#                         states.append(full_grid) # add this potential shape/position to the states list\n",
    "\n",
    "#     # view first 5 shapes \n",
    "#     for i, state in enumerate(states[:5]):\n",
    "#         print(f\"\\nShape {i+1}:\\n{state}\")\n",
    "\n",
    "#     # view last 5 shapes \n",
    "#     for i, state in enumerate(states[-5:]):\n",
    "#         print(f\"\\nShape {len(states) - 5 + i + 1}:\\n{state}\")\n",
    "\n",
    "#     # print number of possible states\n",
    "#     print('\\nNumber of states = ', len(states))\n",
    "    \n",
    "#     return states\n",
    "\n",
    "\n",
    "# # define function to check if tiles are connected in a shape using a flood fill\n",
    "# # used within the generate shapes function\n",
    "# def is_connected(pattern): # takes pattern as argument\n",
    "#     \"\"\"Check if a pattern is connected using flood fill\"\"\"\n",
    "#     if np.sum(pattern) == 0: # return false if the pattern is empty\n",
    "#         return False\n",
    "        \n",
    "#     # find first filled cell\n",
    "#     filled_positions = np.where(pattern == 1) # return coordinates of all 1s in the shape; returns two arrays, the rows and columns where the value is 1\n",
    "#     if len(filled_positions[0]) == 0:\n",
    "#         return False\n",
    "        \n",
    "#     visited = np.zeros_like(pattern, dtype=bool) # tracks the tiles already visited in this round\n",
    "#     stack = [(filled_positions[0][0], filled_positions[1][0])] # start with the first 1 on the grid\n",
    "#     visited[stack[0]] = True\n",
    "#     count = 1 # keeps track of the number of tiles in the shape\n",
    "    \n",
    "#     while stack:\n",
    "#         row, col = stack.pop()\n",
    "#         # check 4-connected neighbors\n",
    "#         for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]: # loops through 4 directions: right, left, down, up; (dr, dc) are the delta changes in row and column for each move\n",
    "#             new_row, new_col = row + dr, col + dc\n",
    "#             if (0 <= new_row < pattern.shape[0] and # makes sure the new row is still within bounds\n",
    "#                 0 <= new_col < pattern.shape[1] and # makes sure the new column is still within bounds\n",
    "#                 pattern[new_row, new_col] == 1 and # only continue if this spot has a 1 aka if it is a tile in the shape\n",
    "#                 not visited[new_row, new_col]): # make sure the given tile has not been visited yet in this go-around\n",
    "#                 visited[new_row, new_col] = True # mark it now as visited to prevent infinite loop\n",
    "#                 stack.append((new_row, new_col)) # add it to the stack to explore it next\n",
    "#                 count += 1 # increase count of tiles in current shape\n",
    "    \n",
    "#     return count == np.sum(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55e32c-5671-46ad-b410-f6f26df3cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to generate the number of possible states \n",
    "all_states = generate_states_like_matlab()\n",
    "\n",
    "n_states = len(all_states)\n",
    "print(f\"Generated {n_states} possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e040eb-dd8b-4019-973d-a0e506edb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dirichlet distribution (dirichlet_counts) \n",
    "\n",
    "# dirichlet_counts holds how many times each shape has been revealed to be the hidden one across all trials so far (plus a starting value of 2 for each shape, as a prior)\n",
    "# it is initialized where all values are equal to 2. this means: “I believe all shapes are equally likely, but I have no real data yet.”\n",
    "# at the end of each trial, the element of the vector corresponding to the shape that was hidden during that trial has 1 added to it, and then the values are re-normalized\n",
    "# the distribution is then more informed about which shape might be hidden\n",
    "# this updated distribution is then what is fed into `get_current_probabilities`, which is used as the distribution in the next trial\n",
    "\n",
    "# the Dirichlet distribution’s length is equal to the total number of possible shapes (i.e., 3904), not just the shapes that have been observed or sampled\n",
    "# the Dirichlet is a prior over the complete hypothesis space. You want to allow for the (small) chance that any of the 3904 shapes could be hidden — not just the handful you've seen\n",
    "# that's what makes this a Bayesian, nonparametric-style inference\n",
    "\n",
    "# EXAMPLE: after revealing shape 2 once, your counts might be: [1.0, 2.0, 1.0, 1.0, 1.0], assuming 5 shapes and shape 2 was seen, making the MAP estimate reflect a slight preference for shape 2 in the next trial\n",
    "# EXAMPLE: if dirichlet_counts = [1, 3, 1, 1, 2], then Shape 1 has been revealed once (just the prior), Shape 2 has been revealed 2 times (plus the prior), and Shape 5 has been revealed once (plus the prior), etc.\n",
    "\n",
    "dirichlet_counts = np.full(n_states, 2)  # each shape starts with count = 2\n",
    "\n",
    "# sanity check\n",
    "print(dirichlet_counts)\n",
    "print(len(dirichlet_counts)) # len should be the number of all possible states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a102b51-b868-4e74-81d6-3d1d13f5b4b2",
   "metadata": {},
   "source": [
    "### Add shape index to df based on state list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feb70a-f518-4a14-9076-6fa06ea70edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add shape ID column based on rows and columns\n",
    "# need this to feed into info analyze_trial function for the hidden shape on a given trial\n",
    "# define the shape set (column, row; starting from top left)\n",
    "shape_patterns = np.zeros((5, 5, 5))\n",
    "\n",
    "# Shape 0 occupies tiles (3,0), (3,1), (4,1), (4,2)\n",
    "shape_patterns[0, 3, 0] = 1\n",
    "shape_patterns[0, 3, 1] = 1\n",
    "shape_patterns[0, 4, 1] = 1\n",
    "shape_patterns[0, 4, 2] = 1\n",
    "\n",
    "# Shape 1 occupies tiles (2,2), (2,3), (2,4), (3,2), (3,4), (4,2), (4,4)\n",
    "shape_patterns[1, 2, 2] = 1\n",
    "shape_patterns[1, 2, 3] = 1\n",
    "shape_patterns[1, 2, 4] = 1\n",
    "shape_patterns[1, 3, 2] = 1\n",
    "shape_patterns[1, 3, 4] = 1\n",
    "shape_patterns[1, 4, 2] = 1\n",
    "shape_patterns[1, 4, 4] = 1\n",
    "\n",
    "# Shape 2 occupies tiles (1,3), (1,4), (2,3), (3,3)\n",
    "shape_patterns[2, 1, 3] = 1\n",
    "shape_patterns[2, 1, 4] = 1\n",
    "shape_patterns[2, 2, 3] = 1\n",
    "shape_patterns[2, 3, 3] = 1\n",
    "\n",
    "# Shape 3 occupies tiles (0,1), (0,2), (1,2), (1,3)\n",
    "shape_patterns[3, 0, 1] = 1\n",
    "shape_patterns[3, 0, 2] = 1\n",
    "shape_patterns[3, 1, 2] = 1 \n",
    "shape_patterns[3, 1, 3] = 1 \n",
    "\n",
    "# Shape 4 occupies tiles (0,0), (0,1), (1,0), (1,1)\n",
    "shape_patterns[4, 0, 0] = 1\n",
    "shape_patterns[4, 0, 1] = 1\n",
    "shape_patterns[4, 1, 0] = 1 \n",
    "shape_patterns[4, 1, 1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca1de5-6e08-4e1d-a192-a8c5c0176db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for shape_id (id of shape in shape set) and true_state_idx (index of state in full list of 3,904 states)\n",
    "exp_choices_df['shape_id'] = np.nan\n",
    "exp_choices_df['true_state_idx'] = np.nan\n",
    "\n",
    "# define to find which shape contains all (col, row) hits\n",
    "def find_shape_id(hit_coords):\n",
    "    for shape_id in range(shape_patterns.shape[0]):\n",
    "        pattern = shape_patterns[shape_id]\n",
    "        if all(pattern[col, row] == 1 for col, row in hit_coords):\n",
    "            return shape_id\n",
    "    return np.nan  \n",
    "\n",
    "# define function to convert hit coords to grid with shape in 1s\n",
    "def make_grid(hit_coords, size=5):\n",
    "    grid = np.zeros((size, size), dtype=int)\n",
    "    for col, row in hit_coords:\n",
    "        grid[row, col] = 1\n",
    "    return grid\n",
    "\n",
    "\n",
    "# loop through each subject & trial group\n",
    "for (subject_id, trial_num), group in exp_choices_df.groupby([\"subject_id\", \"trial_num\"]):\n",
    "    # idenfity hit (MHS) rows and extract hit coordinates from col and row columns\n",
    "    mhs_rows = group[group['choice_outcome'] == 'MHS']\n",
    "    hit_coords = list(zip(mhs_rows['chosen_tile_col'], mhs_rows['chosen_tile_row']))\n",
    "    \n",
    "    if hit_coords:\n",
    "        # find the shape ID by finding the shape id that contains all hit_coords, using find_shape_id function\n",
    "        shape_id = find_shape_id(hit_coords)\n",
    "        \n",
    "        # build binary grid and find match in all_states\n",
    "        grid = make_grid(hit_coords)\n",
    "        true_state_idx = np.nan\n",
    "        for i, state in enumerate(all_states):\n",
    "            if np.array_equal(grid, state):\n",
    "                true_state_idx = i\n",
    "                break\n",
    "\n",
    "        # assign shape_id and true_state_idx to all rows in that trial\n",
    "        mask = (exp_choices_df[\"subject_id\"] == subject_id) & (exp_choices_df[\"trial_num\"] == trial_num)\n",
    "        exp_choices_df.loc[mask, \"shape_id\"] = shape_id\n",
    "        exp_choices_df.loc[mask, \"true_state_idx\"] = true_state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b195e4-3c0c-4141-8fca-c7b1bb002717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify shape set from idices of all possible states\n",
    "print(exp_choices_df.true_state_idx.unique())\n",
    "print('')\n",
    "print(all_states[2450])\n",
    "print('')\n",
    "print(all_states[2753])\n",
    "print('')\n",
    "print(all_states[775])\n",
    "print('')\n",
    "print(all_states[3824])\n",
    "print('')\n",
    "print(all_states[124])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeebb9f-53db-48df-b42a-0b1d24ffaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_choices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675685d-bd91-4caa-830d-faefe80534bf",
   "metadata": {},
   "source": [
    "### Info calculation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adff3d5-e692-4472-aa9f-b32267fdcd56",
   "metadata": {},
   "source": [
    "#### `shannon_entropy`: Shannon entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd44d2-ceb2-4df0-87e1-240e85fbe33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Shannon entropy: H = -sum(p*log(p))\n",
    "def shannon_entropy(probabilities):\n",
    "    nonzero_probs = probabilities[probabilities > 1e-15]  # avoid log(0) \n",
    "    if len(nonzero_probs) == 0:\n",
    "        return 0.0\n",
    "    return -np.sum(nonzero_probs * np.log(nonzero_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e0095-093e-4665-994d-5fd7bb1e0ca9",
   "metadata": {},
   "source": [
    "#### `sm_entropy`: SM entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca65d4-e56a-43ac-9050-f129951984da",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DID NOT END UP USING'''\n",
    "# calculate SM (generalized) entropy: H(r,t) = (1/(t-1)) * (1 - (sum(p^r))^((t-1)/(r-1)))\n",
    "def sm_entropy(probabilities, r=1.1, t=1.1):\n",
    "    if abs(t - 1) < 1e-10 or abs(r - 1) < 1e-10:\n",
    "        return shannon_entropy(probabilities)  # fall back to Shannon\n",
    "    \n",
    "    sum_p_r = np.sum(probabilities ** r)\n",
    "    if sum_p_r <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (1/(t-1)) * (1-(sum_p_r ** ((t-1)/(r-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b31211-0715-46d6-8b88-b82d09d10327",
   "metadata": {},
   "source": [
    "#### `get_initial_shape_priors`: creates the starting beliefs for each subject:\n",
    "\n",
    "- Before any trials: \"I think all 3904 shapes are equally likely\"\n",
    "- Mathematical representation: Uniform probability distribution\n",
    "- Bayesian interpretation: Each shape has been \"observed\" 2 times in our prior experience\n",
    "\n",
    "  \n",
    "MATLAB: shape_counts = ones(1,size(data.patterns,1)) * initial_counts\n",
    "<br>\n",
    "MATLAB: shape_priors = shape_counts./sum(shape_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cacb0-e0a3-4837-bf45-81c983e9098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_shape_priors(n_states, initial_counts):\n",
    "    initial_shape_counts = np.full(n_states, initial_counts) # create vector where each of the states gets the initial value (2)\n",
    "    # represents initial belief that we have \"seen\" each shape twice before, so there is no preference\n",
    "        # initial_counts = the starting 'pseudocount' for each shape (2 for this case)\n",
    "        # n_states = number of total possible stated (3904)\n",
    "    shape_priors = initial_shape_counts/np.sum(initial_shape_counts) \n",
    "        # convert counts into probability distribution by normalizing\n",
    "        # all sum to 1\n",
    "    return shape_priors, initial_shape_counts\n",
    "        # returns both the shape priors (probabilities) AND the shape counts\n",
    "        # shape_priors is used for calculations of info within trials, choice by choice\n",
    "        # initial_shape_counts is used as the starting point for updating between trials with updated info about the hidden shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b260ea-0a3b-4cc1-8da0-edd887233e6d",
   "metadata": {},
   "source": [
    "#### `probb_update`: updates shape probabilities after completing a trial\n",
    "\n",
    "EXAMPLE:\n",
    "Starting counts: [2, 2, 2, 2] (initial_counts = 2)\n",
    "After seeing shape 2: [2, 3, 2, 2]\n",
    "\n",
    "MAP Result:\n",
    "<br>\n",
    "numerator = [2, 3, 2, 2]-1 = [1, 2, 1, 1]\n",
    "denominator = 1+2+1+1=5\n",
    "probabilities = [1/5, 2/5, 1/5, 1/5] = [0.2, 0.4, 0.2, 0.2] <- all of these sum to 1\n",
    "<br>\n",
    "EV Result: probabilities = [2, 3, 2, 2]/9 = [0.222, 0.333, 0.222, 0.222] < still sum to 1\n",
    "<br>\n",
    "**Difference: MAP gives more extreme probabilities (0.4 vs 0.333), EV is more conservative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f299c-296c-4965-98dc-84548ab0b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probb_update(shape_counts, true_state_idx, update_type='MAP'):\n",
    "    ''' ARGS:\n",
    "    patterns: Not actually used here (legacy from MATLAB)\n",
    "    shape_counts: Current \"pseudocounts\" for each shape (how many times we've \"seen\" each)\n",
    "    data_trial: Info about the trial that just finished (contains the true shape)\n",
    "    pattern_flag=2: Always 2 = update only the actual shape that was revealed\n",
    "    update_type='MAP': Use Maximum A Posteriori estimation (vs Expected Value)'''\n",
    "\n",
    "    # create copy to not modify the original \n",
    "    new_shape_counts = shape_counts.copy()\n",
    "\n",
    "    # update the count for the state that matches the true state\n",
    "    new_shape_counts[true_state_idx] += 1.0  # update only the true state (pattern=2 from Dave's og code)\n",
    "\n",
    "    if update_type == 'MAP': # MAP (maximum a posteriori): most likely shapes; what's the most probable distribution given my data?\"\n",
    "        # slightly reduces the influence of the prior (initial counts)\n",
    "        # gives the \"best guess\" distribution\n",
    "        numerator = new_shape_counts - 1.0 # subtract 1 from each count; MAP removes the 'prior' from each count\n",
    "                # EX: [2, 2, 2, ..., 3, ..., 2] becomes [1, 1, 1, ..., 2, ..., 1]\n",
    "        denominator = np.sum(new_shape_counts - 1.0) # sum all of the adjusted counts\n",
    "        shape_posteriors = numerator/denominator # convert to probabilities by normalizeing\n",
    "        shape_posteriors = shape_posteriors/np.sum(shape_posteriors) # renormalize to make sure the probabilities sum to 1 exactly\n",
    "    \n",
    "    elif update_type == 'EV': # EV (expected value): weighted average across all shapes\n",
    "        # EV (Expected Value) - \"Average\" Estimate; what's the expected distribution averaging over all possibilities?\"\n",
    "        # does not subtract 1 from each before normalizing\n",
    "        # keeps more influence from the prior\n",
    "        # use when you want to account for uncertainty in your estimates\n",
    "        shape_posteriors = new_shape_counts/np.sum(new_shape_counts) # simple normalization without subtracting\n",
    "    \n",
    "    return shape_posteriors, new_shape_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe389e-8b3c-4daf-a7b7-9397f48db6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS JUST TRYING TO MAKE COPY OF MATLAB BUT I DONT NEED PATTERN FLAGS HERE SO REVISED ABOVE\n",
    "# def probb_update(patterns, shape_counts, data_trial, pattern_flag=2, update_type='MAP'):\n",
    "#     ''' ARGS:\n",
    "#     patterns: Not actually used here (legacy from MATLAB)\n",
    "#     shape_counts: Current \"pseudocounts\" for each shape (how many times we've \"seen\" each)\n",
    "#     data_trial: Info about the trial that just finished (contains the true shape)\n",
    "#     pattern_flag=2: Always 2 = update only the actual shape that was revealed\n",
    "#     update_type='MAP': Use Maximum A Posteriori estimation (vs Expected Value)'''\n",
    "\n",
    "#     # create copy to not modify the original \n",
    "#     new_shape_counts = shape_counts.copy()\n",
    "\n",
    "#     # if pattern flag = 2 (always here); from the original code, pattern 2 just updates the true shape that was hidden, not all consistent shapes\n",
    "#     if pattern_flag == 2:  \n",
    "#         # MATLAB: board = zeros(1,data.r*data.c); board(data.trial(trial).targ_shape) = 1;\n",
    "#         # find which pattern matches the true shape\n",
    "#         true_state_idx = data_trial['true_state_idx']\n",
    "#         new_shape_counts[true_state_idx] += 1.0 # add 1 to the index of the state that was observed (aka the shape that was hidden)\n",
    "        \n",
    "#         if update_type == 'MAP': # MAP (maximum a posteriori): most likely shapes\n",
    "#             numerator = new_shape_counts - 1.0 # subtract 1 from each count; MAP removes the 'prior' from each count\n",
    "#                 # EX: [2, 2, 2, ..., 3, ..., 2] becomes [1, 1, 1, ..., 2, ..., 1]\n",
    "#             denominator = np.sum(new_shape_counts - 1.0) # sum all of the adjusted counts\n",
    "#             shape_posteriors = numerator/denominator # convert to probabilities by normalizeing\n",
    "#             shape_posteriors = shape_posteriors/np.sum(shape_posteriors) # renormalize to make sure the probabilities sum to 1 exactly\n",
    "            \n",
    "#         elif update_type == 'EV': # EV (expected value): weighted average across all shapes\n",
    "#             shape_posteriors = new_shape_counts/np.sum(new_shape_counts) # simple normalization without subtracting\n",
    "    \n",
    "#     return shape_posteriors, new_shape_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e48029-78cf-4725-8816-c91415e8ec31",
   "metadata": {},
   "source": [
    "#### `all_expected_infos`: calculate and store expected information for each tile available to be chosen\n",
    "- For each tile you could choose next, calculates how much information you'd expect to gain\n",
    "- Calculate expected information for each remaining tile\n",
    "- Returns both the EI values and the remaining_tiles list for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823f75a-078b-4bda-8154-333e85bd02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_expected_infos(current_probs, remaining_tiles, all_states):\n",
    "    \"\"\"\n",
    "    Based on MATLAB logic:\n",
    "    for k=1:numel(remaining_tiles)\n",
    "        for o=0:1 % outcome (miss=0, hit=1)\n",
    "            li_next = zeros(1,size(patterns,1));\n",
    "            li_next(o == patterns(:,remaining_tiles(k))) = 1;\n",
    "            square_probbs_next = square_probbs.*li_next;\n",
    "            if sum(square_probbs_next) == 0\n",
    "                H_next(o+1) = H_S;\n",
    "            else\n",
    "                square_probbs_next = square_probbs_next./sum(square_probbs_next);\n",
    "                H_next(o+1) = -sum(nonzeros(square_probbs_next).*log(nonzeros(square_probbs_next)));\n",
    "            end\n",
    "        end\n",
    "        EI(k) = H_S - mean(H_next);\n",
    "    end\n",
    "    \"\"\"\n",
    "    \"\"\" ARGs:\n",
    "    current_probs: current beliefs about which shape is hidden (probability for each of 3904 shapes)\n",
    "    remaining_tiles: list of tiles you haven't chosen yet (e.g., [0, 1, 3, 5, 7, ...])\n",
    "    all_states: All 3904 possible shapes\"\"\"\n",
    "    \n",
    "    H_S = shannon_entropy(current_probs) # calculate the current Shannon entropy\n",
    "    EI = np.zeros(len(remaining_tiles)) # create an array the length of remaining tiles that will store expected info for each remaining tile\n",
    "\n",
    "    for k, tile_idx in enumerate(remaining_tiles): # for each remaining tile\n",
    "        # convert linear tile index to row, col\n",
    "        # matlab used linear indexing by default\n",
    "        # linear index is a way to represent 2D grid positions using a single number instead of (row, col) coordinates. more efficient and reduces errors.\n",
    "            # grid with (row, col) coordinates:\n",
    "            # (0,0) (0,1) (0,2) (0,3) (0,4)\n",
    "            # (1,0) (1,1) (1,2) (1,3) (1,4)\n",
    "            # (2,0) (2,1) (2,2) (2,3) (2,4)\n",
    "            # (3,0) (3,1) (3,2) (3,3) (3,4)\n",
    "            # (4,0) (4,1) (4,2) (4,3) (4,4)\n",
    "            \n",
    "            # grid with linear indices:\n",
    "            #  0   1   2   3   4\n",
    "            #  5   6   7   8   9\n",
    "            # 10  11  12  13  14\n",
    "            # 15  16  17  18  19\n",
    "            # 20  21  22  23  24\n",
    "\n",
    "            # to convery from (row,col) to linear index: linear_index = row*5 + col\n",
    "                # 0,4) → 0*5 + 4 = 4\n",
    "                # (2,3) → 2*5 + 3 = 13\n",
    "                # (4,4) → 4*5 + 4 = 24\n",
    "            # to convert from linear index to row,col, use row, col = divmod(linear_index, 5)\n",
    "        row, col = divmod(tile_idx, 5)  \n",
    "        \n",
    "        H_next = np.zeros(2)  # make array to store outcomes 0 (miss) and 1 (hit)\n",
    "            # H_next[0] = entropy if you get a MISS\n",
    "            # H_next[1] = entropy if you get a HIT\n",
    "\n",
    "        # simulate both outcome types, hit or miss\n",
    "        for outcome in [0, 1]: # run the following simulation twice, once for a miss and once for a hit\n",
    "            # create likelihood vector; an array of 0s, one for each possible shape\n",
    "                # for round 1 (misses) mark which shapes are consistent with getting a miss\n",
    "                # for round 2 (hits) mark which shapes are consistent with getting a hit\n",
    "            likelihood_next = np.zeros(len(all_states))\n",
    "            \n",
    "            # MATLAB: li_next(o == patterns(:,remaining_tiles(k))) = 1\n",
    "            for i, state in enumerate(all_states):\n",
    "                tile_value = state[row, col]\n",
    "                if outcome == tile_value: # if the tile value is consistent with the outcome for that round (hit/miss)\n",
    "                    likelihood_next[i] = 1.0 # mark a 1 down to represent that\n",
    "                # do this for all shapes\n",
    "            # result after MISS simulation:\n",
    "                # likelihood_next could be something like [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...]\n",
    "                # interpretation:                \n",
    "                    # 1.0 = \"This shape has an empty tile at (2,3), so it could give a MISS\"\n",
    "                    # 0.0 = \"This shape has a filled tile at (2,3), so it would give a HIT, not a MISS\"\n",
    "            # result after HIT simulation:\n",
    "                # likelihood_next could be something like [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...]\n",
    "                # interpretation:\n",
    "                    # 1.0 = \"This shape has a filled tile at (2,3), so it could give a HIT\"\n",
    "                    # 0.0 = \"This shape has an empty tile at (2,3), so it would give a MISS, not a HIT\"\n",
    "            # notice how hits and misses outcomes will be opposite\n",
    "\n",
    "            \n",
    "            # update probabilities: apply Bayesian updating - multiply current beliefs by likelihood\n",
    "            square_probbs_next = current_probs*likelihood_next\n",
    "\n",
    "            if np.sum(square_probbs_next) == 0: # if no shapes are consistent with this outcome\n",
    "                # MATLAB: H_next(o+1) = H_S\n",
    "                H_next[outcome] = H_S # assume no infogain; info stays the same\n",
    "            else: # otherwise (if some shapes are consistent)\n",
    "                # MATLAB: square_probbs_next = square_probbs_next./sum(square_probbs_next)\n",
    "                square_probbs_next = square_probbs_next/np.sum(square_probbs_next) # normalize probs so they sum to 1\n",
    "                # then calculate entropy\n",
    "                H_next[outcome] = shannon_entropy(square_probbs_next)\n",
    "        \n",
    "        # using that, calculate the expected information by subtracting the average future uncertainty from the current uncertainty\n",
    "        # MATLAB: EI(k) = H_S - mean(H_next)\n",
    "        EI[k] = H_S - np.mean(H_next)\n",
    "    \n",
    "    return EI, remaining_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d47e2-1ed6-4d68-addb-fa8268ab338d",
   "metadata": {},
   "source": [
    "#### `analyze_trial_infos`: analyze trial; based on MATLAB choice_regressions_function code\n",
    "This is the main trial analysis function - it processes one complete trial and calculates both expected and historical information for each choice. and retains the entire EI landscape for all possible choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d41b1-434d-4662-9b1c-04cbe5eb72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trial_infos(choices, outcomes, true_state_idx, initial_belief_probs, all_states):\n",
    "    \n",
    "    ''' ARGS\n",
    "    choices: List of (row, col) positions chosen, e.g., [(3,1), (2,2), (3,2), ...]\n",
    "    outcomes: List of True/False for hit/miss, e.g., [True, False, False, ...]\n",
    "    true_state_idx: the index of which of the 3904 shapes was actually hidden\n",
    "    initial_belief_probs: Starting probabilities for this trial (from Dirichlet)\n",
    "    all_states: All 3904 possible shapes ''' \n",
    "    \n",
    "    square_probbs = initial_belief_probs.copy() # make a copy of starting beliefs that will be updated throughout the trial\n",
    "                                                # we don't want to update the actual probs with in-trial info, so copy\n",
    "    # create place to store trial info\n",
    "    # each list gets one entry per choice in the trial\n",
    "    trial_info = {\n",
    "        'S_info': [], # historical information (what was gained)\n",
    "        'S_exp_info': [], # expected information (what was expected to be gained)\n",
    "        'reward': [], # actual rewards\n",
    "        'exp_reward': [], # expected rewards (not implemented here)\n",
    "        'choices': choices, # store choices for later use\n",
    "        'EI_landscapes': [], # store complete EI landscape for each choice; aka the expected info for each option\n",
    "        'remaining_tiles_list': []  # store which tiles were available for each choice\n",
    "    }\n",
    "    \n",
    "    # count the number of choices in the given trial\n",
    "    n_choices = len(choices)\n",
    "\n",
    "    for j in range(n_choices): # for each choice in the trial\n",
    "        choice_coords = choices[j] # loop through each choice's [row,col] position\n",
    "        outcome = outcomes[j] # and grab whether it was a HIT or a MISS\n",
    "\n",
    "        # calculate the CURRENT entropy\n",
    "        # MATLAB: H_S(1) = -sum(nonzeros(square_probbs).*log(nonzeros(square_probbs)))\n",
    "        H_S_before = shannon_entropy(square_probbs)\n",
    "        \n",
    "        # calculate expected information for all remaining tiles\n",
    "        if j == 0: # if we are just on the first choice, and no choices have been made, \n",
    "            # MATLAB: remaining_tiles = 1:25\n",
    "            remaining_tiles = list(range(25))  # all tiles remain, so length is 25\n",
    "        else:\n",
    "            # if not the first choice, \n",
    "            # MATLAB: remaining_tiles = setdiff(1:25, data.trial(i).targ_order(1:j-1))\n",
    "            # pull out the linear indices of the tiles that have been chosen already\n",
    "            chosen_so_far = [choices[k][0]*5 + choices[k][1] for k in range(j)]  # convert to linear indices (see formular above)\n",
    "                # k in range(j) just means for each index in the number of choices made so far; on the jth choice\n",
    "            # the remaining tiles are those not in chosen_so_far\n",
    "            remaining_tiles = [t for t in range(25) if t not in chosen_so_far]\n",
    "        \n",
    "        # now, calculate the expected information for all remaining tiles (NOT those previously chosen in the trial already)\n",
    "        # EI is an array of expected info values; tells you how informative each possible choice would be\n",
    "        EI, remaining_tiles_returned = all_expected_infos(square_probbs, remaining_tiles, all_states)\n",
    "\n",
    "        # store the complete EI landscape\n",
    "        trial_info['EI_landscapes'].append(EI.copy())\n",
    "        trial_info['remaining_tiles_list'].append(remaining_tiles_returned.copy())\n",
    "        \n",
    "        # get expected info for the actual choice that was made\n",
    "        current_tile_linear = choice_coords[0]*5 + choice_coords[1]  # convert (row, col) to linear index\n",
    "        # need to match up 2 diff lists that are parallel, correspond to one another by position (idx)\n",
    "            # remaining_tiles = list of tiles to choose from\n",
    "            # EI = list of expected info gain from each of those available tiles\n",
    "        if current_tile_linear in remaining_tiles: \n",
    "            tile_position = remaining_tiles.index(current_tile_linear) # find the idx position of the tile in the remaining tiles list\n",
    "            expected_info = EI[tile_position] # match that to the idx of the EI list to get the expected info of that tile\n",
    "        else:\n",
    "            expected_info = 0.0  # safety check; this shouldn't happen, so red flag if you get it as 0\n",
    "\n",
    "        \n",
    "        # update beliefs (square_probbs) based on the actual choice outcome\n",
    "        # MATLAB: li = zeros(1,size(patterns,1)); li(rwd_order(j) == patterns(:,targ_order(j))) = 1;\n",
    "        likelihood_vec = np.zeros(len(all_states)) # make empty likeihood vector\n",
    "        for i, state in enumerate(all_states): # loop through every possible state\n",
    "            tile_value = state[choice_coords[0], choice_coords[1]] # look at whether the tile you just chose is empty or filled in each possible state\n",
    "            if (outcome and tile_value == 1) or (not outcome and tile_value == 0): # if the state is consistent with your outcome (e.g., if the current state in all states had that tile empty, and you just got a miss)\n",
    "                likelihood_vec[i] = 1.0 # give it a 1 (1 showing outcome of this choice being consistent with the current state in all states)\n",
    "            # basically a  check on whether this current state that you are on could have produced the outcome you just observed\n",
    "        # now, apply Bayesian updating\n",
    "            # multiply your list of beliefs (square_probs) by the likelihood vector\n",
    "            # since states inconsistent with the observed outcome were assigned a 0, these states will be eliminated from the current set of beliefs\n",
    "        square_probbs = square_probbs*likelihood_vec\n",
    "        square_probbs = square_probbs/np.sum(square_probbs) # after updating, renormalize the list of beliefs\n",
    "\n",
    "        # calculate historical information (what was actually gained)\n",
    "            # difference in shannon entropy before and after the choice\n",
    "        H_S_after = shannon_entropy(square_probbs)\n",
    "        historical_info = -(H_S_after - H_S_before)  # MATLAB: -diff(H_S)\n",
    "        \n",
    "        # store results for both types of info for that trial\n",
    "        trial_info['S_info'].append(historical_info)\n",
    "        trial_info['S_exp_info'].append(expected_info)\n",
    "        trial_info['reward'].append(1.0 if outcome else 0.0)\n",
    "        trial_info['exp_reward'].append(0.0)  # placeholder - would need reward history\n",
    "        \n",
    "        # update tile tracking -- keeping running totals of information gained from each tile and the times that tile has been chosen\n",
    "        # keeping these in but not actually using rn in the code\n",
    "        row, col = choice_coords\n",
    "        tile_information_sums[row, col] += historical_info\n",
    "        tile_choice_counts[row, col] += 1\n",
    "    \n",
    "    return trial_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141f791-47b2-469e-90c1-07cf1a9a7cd3",
   "metadata": {},
   "source": [
    "#### `process_info_all_trials`: process all trials for historical and expected information for all subjects\n",
    "Matching MATLAB's `choice_regressions_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ee466-9cfa-4350-b7be-6460e13ce40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_info_all_trials(exp_choices_df, all_states):\n",
    "    '''ARGS\n",
    "    exp_choices_df: the df with all experimental data\n",
    "    all_states: all 3904 possible states'''\n",
    "    n_states = len(all_states)\n",
    "    subjects_data = [] # create empty list to store subject data\n",
    "    \n",
    "    for subject in exp_choices_df['subject_id'].unique(): # for each subject\n",
    "        subject_df = exp_choices_df[exp_choices_df['subject_id'] == subject] # pull out their data from the main df\n",
    "        \n",
    "        # initialize the dirichlet and shape counts for this subject\n",
    "        shape_priors, shape_counts = get_initial_shape_priors(n_states, initial_counts)\n",
    "\n",
    "        # make empty lists to store this subject's trial data and info\n",
    "        subject_trial_info = [] # store detailed analysis for each trial\n",
    "        all_choices = [] # store every choice made as linear indices\n",
    "        all_choice_nums = [] # store choice number within each trial\n",
    "        all_trial_nums = [] # store which trial each choice belongs to\n",
    "\n",
    "        # now loop through each trial for this subject\n",
    "        for trial_num in sorted(subject_df['trial_num'].unique()):\n",
    "            # get all choices for this specific trial\n",
    "            trial_df = subject_df[subject_df['trial_num'] == trial_num].sort_values('choice_num')\n",
    "            \n",
    "            # extract trial data\n",
    "            # convert df columns to list of coordinate tuples; zip combines row and column into coordinate pairs\n",
    "            choices = list(zip(trial_df['chosen_tile_row'].astype(int), \n",
    "                              trial_df['chosen_tile_col'].astype(int)))\n",
    "            # MHS/hit -> True, MMS/miss -> False\n",
    "            outcomes = trial_df['choice_outcome'].map({'MHS': True, 'MMS': False}).tolist()\n",
    "            # get which shape was actually hidden this trial\n",
    "            # just take the first index since all rows in this trial have the same value in this column of the main df\n",
    "            true_state_idx = int(trial_df['true_state_idx'].iloc[0])\n",
    "            \n",
    "            # analyze this trial using the analyze_trial_infos function to find infos\n",
    "            trial_info = analyze_trial_infos(choices, outcomes, true_state_idx, shape_priors.copy(), all_states)\n",
    "            subject_trial_info.append(trial_info) # add this trial's data to subject data\n",
    "            \n",
    "            # convert choice coordinates back to linear indices for storage\n",
    "            all_choices.extend([choice[0]*5 + choice[1] for choice in choices])  # Linear indices\n",
    "            # add choice nums for this trial; using extend to add each element individually\n",
    "            all_choice_nums.extend(range(1, len(choices) + 1))\n",
    "            # add the trial number for each of the choice nums\n",
    "            all_trial_nums.extend([trial_num]*len(choices))\n",
    "            \n",
    "            # after the trial is complete, update the shape_counts and shape_priors \n",
    "            data_trial = {'true_state_idx': true_state_idx}\n",
    "            shape_priors, shape_counts = probb_update(shape_counts, true_state_idx, update_type='MAP')\n",
    "        \n",
    "        # store data for this subject's full performance\n",
    "        subjects_data.append({\n",
    "            'subject_id': subject,\n",
    "            'trial_info': subject_trial_info,\n",
    "            'choice': all_choices,\n",
    "            'choice_num': all_choice_nums,\n",
    "            'trial_num': all_trial_nums\n",
    "        })\n",
    "    \n",
    "    return subjects_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceec280-7635-4a00-8335-d634b96d628b",
   "metadata": {},
   "source": [
    "#### `create_expected_infos_df`: Create DataFrame with expected information for ALL possible tiles for each choice.\n",
    "\n",
    "- One row per choice\n",
    "- Columns for subject_id, trial_num, choice_num, chosen_tile_row, chosen_tile_col, info_chosen\n",
    "- 25 columns (info_tile_0_0 through info_tile_4_4) with expected info for each tile\n",
    "- NaN for tiles that were already chosen (not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa6631-0574-4a8e-91fb-894087bc738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expected_infos_df(subjects_data):\n",
    "\n",
    "    rows = [] # create empty list that will store one dictionary per choice; each dictionary becomes a df row\n",
    "    \n",
    "    for subject_data in subjects_data: # for each set of subject data\n",
    "        subject_id = subject_data['subject_id'] # pull subject_id\n",
    "        \n",
    "        # create sorted list of unique trial numbers for this subject\n",
    "        trial_nums = sorted(set(subject_data['trial_num']))\n",
    "\n",
    "        # for each trial\n",
    "        for trial_idx, trial_info in enumerate(subject_data['trial_info']):\n",
    "            trial_num = trial_nums[trial_idx] # pull trial number\n",
    "            choices = trial_info['choices'] # get the list of (row, col) coordinates chosen in this trial\n",
    "\n",
    "            # for each choice in the current trial\n",
    "            for choice_idx in range(len(choices)):\n",
    "                choice = choices[choice_idx] # get the specific (row, col) coordinates for this choice\n",
    "                choice_num = choice_idx + 1 # convert 0-based index to 1-based choice number.. so choice num starts at 1 not 0\n",
    "                chosen_tile_row, chosen_tile_col = choice # unpack choice coordinates into separate variables\n",
    "                \n",
    "                # get the EI landscape and remaining tiles for this choice\n",
    "                EI = trial_info['EI_landscapes'][choice_idx]\n",
    "                remaining_tiles = trial_info['remaining_tiles_list'][choice_idx]\n",
    "                \n",
    "                # create the dictionary for this choice row\n",
    "                row_dict = {\n",
    "                    'subject_id': subject_id,\n",
    "                    'trial_num': trial_num,\n",
    "                    'choice_num': choice_num,\n",
    "                    'chosen_tile_row': chosen_tile_row,\n",
    "                    'chosen_tile_col': chosen_tile_col,\n",
    "                }\n",
    "                \n",
    "                # create EI columns for each tile\n",
    "                # create 25 columns (one for each grid position) and set all to null initially\n",
    "                for col in range(5):\n",
    "                    for row in range(5):\n",
    "                        row_dict[f'info_tile_{col}_{row}'] = np.nan\n",
    "                \n",
    "                # fill in the expected information for available tiles\n",
    "                for k, tile_linear_idx in enumerate(remaining_tiles): # loop through tiles that were available for this choice\n",
    "                    tile_row, tile_col = divmod(tile_linear_idx, 5) # convert linear index back to (row, col) coordinates\n",
    "                    row_dict[f'info_tile_{tile_col}_{tile_row}'] = EI[k] # set the expected info value for this tile position\n",
    "                \n",
    "                # set info_chosen to the expected info for the chosen tile\n",
    "                chosen_tile_linear = chosen_tile_row*5 + chosen_tile_col # convert chosen coordinates to linear index\n",
    "                if chosen_tile_linear in remaining_tiles: # check if this tile was actually available (should always be true)\n",
    "                    chosen_tile_position = remaining_tiles.index(chosen_tile_linear) # find index of this tile in remaining tiles list\n",
    "                    row_dict['info_chosen'] = EI[chosen_tile_position] # pull EI for that tile\n",
    "                else:\n",
    "                    row_dict['info_chosen'] = 0.0  # safety fallback\n",
    "\n",
    "                # add this choice's row\n",
    "                rows.append(row_dict)\n",
    "    # return df of rows\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e88bd-661d-4cab-8863-3aed8f82186a",
   "metadata": {},
   "source": [
    "#### `create_regression_df`: create a df with the info trial-wise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0d499-ff2f-45cd-9ef1-8ac5105d6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_df(subjects_data):\n",
    "    rows = [] # list to store all subjet dicts\n",
    "    for subject_data in subjects_data:\n",
    "        subject_id = subject_data['subject_id']\n",
    "        \n",
    "        # flatten all expected info and rewards across trials -- converts from nested to lists\n",
    "        all_exp_info = []\n",
    "        all_historical_info = []\n",
    "        for trial_info in subject_data['trial_info']:\n",
    "            all_exp_info.extend(trial_info['S_exp_info'])\n",
    "            all_historical_info.extend(trial_info['S_info'])\n",
    "        \n",
    "        # iterate through the subjects trials and choices lists and add their data\n",
    "        for i, (choice_num, trial_num) in enumerate(zip(subject_data['choice_num'], subject_data['trial_num'])):\n",
    "            if i < len(all_exp_info):  \n",
    "                rows.append({\n",
    "                    'subject_id': subject_id,\n",
    "                    'trial_num': trial_num,\n",
    "                    'choice_num': choice_num,\n",
    "                    'tile_linear_idx': subject_data['choice'][i],\n",
    "                    'tile_row': subject_data['choice'][i]//5,\n",
    "                    'tile_col': subject_data['choice'][i]%5,\n",
    "                    'expected_info': all_exp_info[i],\n",
    "                    'historical_info': all_historical_info[i]\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2482d-4bc2-40f9-898d-ca51e4738e7a",
   "metadata": {},
   "source": [
    "#### `run_info_analysis`: run full info analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829cea5-2151-4af6-861b-5c033b11765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_info_analysis(exp_choices_df, all_states):\n",
    "    print(f\"Processing {len(all_states)} states for {len(exp_choices_df['subject_id'].unique())} subjects...\")\n",
    "    \n",
    "    # process all subjects\n",
    "    subjects_data = process_info_all_trials(exp_choices_df, all_states)\n",
    "    \n",
    "    # create regression df, matching the output of dave's code\n",
    "    regression_df = create_regression_df(subjects_data)\n",
    "\n",
    "    # create all expected infos df\n",
    "    expected_infos_df = create_expected_infos_df(subjects_data)\n",
    "\n",
    "    # print info about output\n",
    "    print(f\"Completed analysis. First few expected info values:\")\n",
    "    print(f\"Regression DataFrame shape: {regression_df.shape}\")\n",
    "    print(f\"Expected Infos DataFrame shape: {expected_infos_df.shape}\")\n",
    "    \n",
    "    print(f\"\\nFirst few regression values:\")\n",
    "    print(regression_df[['subject_id', 'trial_num', 'choice_num', 'expected_info']].head(10))\n",
    "    \n",
    "    print(f\"\\nFirst row of expected infos (showing non-NaN columns):\")\n",
    "    first_row = expected_infos_df.iloc[0]\n",
    "    non_nan_tiles = {col: val for col, val in first_row.items() \n",
    "                     if col.startswith('info_tile_') and not pd.isna(val)}\n",
    "    print(f\"Available tiles for first choice: {len(non_nan_tiles)} tiles\")\n",
    "    print(f\"Sample values: {dict(list(non_nan_tiles.items())[:5])}\")\n",
    "    \n",
    "    return subjects_data, regression_df, expected_infos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af41a4-3939-47d0-b10c-628972ad0ff4",
   "metadata": {},
   "source": [
    "### Calculate information with functions\n",
    "- call all of these functions to get the info results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf97e7-a4d9-4620-a714-bace5d775555",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data, regression_df, expected_infos_df = run_info_analysis(exp_choices_df, all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ae014-43e6-42f3-bf4e-3a69ac247cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "# load in already calculated infos\n",
    "expected_infos_df=pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/analysis/expected_infos_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56213328-e17c-410d-9def-02c9db53951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_infos_df\n",
    "#expected_infos_df.to_csv('expected_infos_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbac2f7-17ae-4da9-b465-59ea17e6ba2a",
   "metadata": {},
   "source": [
    "### (not used)  o.g. code that calculates straight entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad12d12-4f54-4faf-aa4e-92d709b0ec4f",
   "metadata": {},
   "source": [
    "#### (not used) `get_current_probabilities`: get current probabilities for each state from Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc6fe5-b4ad-4a5b-ac33-baf2905fd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get current state probabilities using maximum a posteriori (MAP) estimate from Dirichlet\n",
    "# # Dirichlet distribution is a way to model uncertainty over probabilities of multiple outcomes (like shape 1, shape 2, ... shape K)\n",
    "# def get_current_probabilities():\n",
    "#     # MAP estimate (a point estimate of the most probable distribution) for the probability of category  is: (alpha_i - 1) / (sum(alpha) - K)\n",
    "#     # apply a MAP correction to each Dirichlet α value, ensuring non-negativity \n",
    "#     # dirichlet_counts is updated at the end of each trial by adding 1 to the state that was in that trial (aka the shape that was hidden)\n",
    "#     adjusted_counts = np.maximum(dirichlet_counts-1, 0)\n",
    "#         # dirichlet_counts is a vector of counts (the α values in a Dirichlet distribution) for each shape/category\n",
    "#         # subtracting 1 from each count in the vector of counts implements a MAP correction\n",
    "#     # adjusted_counts then is the corrected MAP numerator value\n",
    "#     total = np.sum(adjusted_counts) # total is the sum of all counts that have been MAP adjusted, for denominator in renormalization\n",
    "#     # return uniform probabilities if counts are all zero (shouldn't happen if priors are > 0)\n",
    "#     if total == 0:\n",
    "#         return np.ones(n_states)/n_states\n",
    "#     # return the normalized adjusted counts for the updated probability distribution\n",
    "#     return adjusted_counts/total # this is `shape_priors` in dave's code I think\n",
    "\n",
    "# # so the output of this function is `shape_priors` from dave's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3d586-4b38-433a-a11b-e6bddc9c6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check\n",
    "# square_probbs = get_current_probabilities().copy()\n",
    "# print(square_probbs)\n",
    "# print(len(square_probbs))\n",
    "# print(square_probbs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb777c-74af-4ef4-9921-d13a67d7899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add one to the state that was hidden\n",
    "# dirichlet_counts[true_state_idx] += 1\n",
    "# dirichlet_counts\n",
    "\n",
    "# # rerunning the sanity check now should update the probability in the distribution for the second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5471f-e882-4594-9a98-eca4a1363ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # name the state/shape that was hidden the one at index 2 in the list of all possible states\n",
    "# true_state_idx=2\n",
    "# true_state_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf27c74-5a93-4b3d-8788-4d203d1e276c",
   "metadata": {},
   "source": [
    "#### (not used) `analyze_trial`: analyze info gained in each choice, update dirichlet at end of each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae190620-5df1-49e4-893c-1e77a6ae9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to analyze a full trial\n",
    "# ''' don't think i need to include parameters since i believe they are for SM entropy only but tbd'''\n",
    "# def analyze_trial(choices, outcomes, true_state_idx, entropy_type='shannon', r=1.1, t=1.1):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         choices: list of (row, col) tile positions (corresponds to targ_order)\n",
    "#         outcomes: list of True/False for hit/miss (corresponds to rwd_order)\n",
    "#         true_state_idx: index of the actual state revealed\n",
    "#         entropy_type: 'shannon' or 'sm' for generalized entropy\n",
    "#         r, t: parameters for SM entropy (generalized infotaxis)\n",
    "    \n",
    "#     Returns:\n",
    "#         dictionary with trial results\n",
    "#     \"\"\"\n",
    "#     # need to access variables outside of the function itself\n",
    "#     global dirichlet_counts, tile_information_sums, tile_choice_counts\n",
    "    \n",
    "#     # start with the current Dirichlet counts (with MAP correction) to get the initial belief state B\n",
    "#     square_probbs = get_current_probabilities().copy()\n",
    "    \n",
    "#     info_gain = []\n",
    "#     sm_info_gain = []\n",
    "#     belief_states = [square_probbs.copy()]\n",
    "#     shannon_entropies = [shannon_entropy(square_probbs)]\n",
    "#     sm_entropies = [sm_entropy(square_probbs, r, t)]\n",
    "    \n",
    "#     # process each choice \n",
    "#     # for each choice and its outcome\n",
    "#     for choice, outcome in zip(choices, outcomes):\n",
    "#         # calculate entropy before choice (calculating both shannon and SM for now)\n",
    "#         H_S_before = shannon_entropy(square_probbs)\n",
    "#         H_SM_before = sm_entropy(square_probbs, r, t)\n",
    "        \n",
    "#         # update belief state aka vector of probabilities after each choice - figure out which shapes are still possible given the outcome\n",
    "#         # create likelihood vector\n",
    "#         row, col = choice\n",
    "#         likelihood_vec = np.zeros(n_states)\n",
    "#         for i, state in enumerate(all_states): # for each state\n",
    "#             tile_filled = state[row, col] == 1\n",
    "#             # if you clicked on a tile and got a hit, only keep shapes where that tile is filled\n",
    "#             # if you got a miss, only keep shapes where that tile is empty\n",
    "#             if (outcome and tile_filled) or (not outcome and not tile_filled):\n",
    "#                 likelihood_vec[i] = 1.0 \n",
    "#             else:\n",
    "#                 likelihood_vec[i] = 0.0\n",
    "        \n",
    "#         # update probabilities aka belief: multiply by likelihood and normalize\n",
    "#         square_probbs = square_probbs*likelihood_vec\n",
    "#         square_probbs = square_probbs/np.sum(square_probbs)  # normalize\n",
    "#         # now square_probbs = the probabilities AFTER that choice and will be used for the next choice and post-choice entropy calcs\n",
    "        \n",
    "#         # calculate entropy after updating the vector of probabilities \n",
    "#         H_S_after = shannon_entropy(square_probbs)\n",
    "#         H_SM_after = sm_entropy(square_probbs, r, t)\n",
    "        \n",
    "#         # calculate information gain (negatives to make the value positive, like in og matlab code)\n",
    "#         shannon_info = -(H_S_after-H_S_before)  # -diff(H_S)\n",
    "#         sm_info = -(H_SM_after-H_SM_before)     # -diff(H_SM)\n",
    "        \n",
    "#         info_gain.append(shannon_info)\n",
    "#         sm_info_gain.append(sm_info)\n",
    "        \n",
    "#         # update tile tracking for expected information -- how informative that specific tile was for this choice\n",
    "#         tile_information_sums[row, col] += shannon_info\n",
    "#         tile_choice_counts[row, col] += 1\n",
    "        \n",
    "#         # store results\n",
    "#         belief_states.append(square_probbs.copy())\n",
    "#         shannon_entropies.append(H_S_after)\n",
    "#         sm_entropies.append(H_SM_after)\n",
    "    \n",
    "#     # after the trial is over, update the Dirichlet counts \n",
    "#     dirichlet_counts[true_state_idx] += 1\n",
    "\n",
    "#     # return all results\n",
    "#     return {\n",
    "#         'choices': choices,\n",
    "#         'outcomes': outcomes, \n",
    "#         'info_gain': info_gain, # Shannon info (S_info in MATLAB)\n",
    "#         'sm_info_gain': sm_info_gain, # SM info (SM_info in MATLAB)\n",
    "#         'belief_states': belief_states,\n",
    "#         'shannon_entropies': shannon_entropies,\n",
    "#         'sm_entropies': sm_entropies,\n",
    "#         'total_information': sum(info_gain),\n",
    "#         'total_sm_information': sum(sm_info_gain)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba144d-329c-445f-b96e-27ca88b4adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create empty list to store dictionaries of trial results\n",
    "# trial_results = []\n",
    "\n",
    "# for subject in exp_choices_df['subject_id'].unique():\n",
    "#     subject_df = exp_choices_df[exp_choices_df['subject_id'] == subject]\n",
    "\n",
    "#     # reset Dirichlet counts for each new subject (before starting trial things)\n",
    "#     dirichlet_counts = np.full(n_states, 2.0)  # same as in your function\n",
    "    \n",
    "#     for trial_num in subject_df['trial_num'].unique():\n",
    "#         trial_df = subject_df[subject_df['trial_num'] == trial_num].sort_values('choice_num')\n",
    "\n",
    "#         # extract choice coords as [row,col], since states are in numpy arrays, which expect to be accessed with numpy indexing, aka row, col\n",
    "#         choices = list(zip(trial_df['chosen_tile_row'].astype(int), trial_df['chosen_tile_col'].astype(int)))\n",
    "        \n",
    "#         # extract choices  outcomes (True for hits, False for misses)        \n",
    "#         outcomes = trial_df['choice_outcome'].map({'MHS': True, 'MMS': False}).tolist()\n",
    "        \n",
    "#         # extract the true_state_idx, or the state out of all 3904 possible states of the hidden shape\n",
    "#         true_state_idx = int(trial_df['true_state_idx'].iloc[0])\n",
    "\n",
    "#         # calculate info for the trial\n",
    "#         result = analyze_trial(choices, outcomes, true_state_idx)\n",
    "\n",
    "#         # attach additional data to be appended\n",
    "#         result['subject_id'] = subject\n",
    "#         result['trial_num'] = trial_num\n",
    "#         trial_results.append(result)\n",
    "\n",
    "# # output = list of dictionaries storing trial info and info gained per trial\n",
    "\n",
    "# # convert that into a df\n",
    "# rows = []\n",
    "# for trial in trial_results:\n",
    "#     for i, (choice, info_gain) in enumerate(zip(trial['choices'], trial['info_gain'])):\n",
    "#         rows.append({\n",
    "#             'subject_id': trial['subject_id'],\n",
    "#             'trial_num': trial['trial_num'],\n",
    "#             'choice_num': i + 1,\n",
    "#             'tile_row': choice[0],\n",
    "#             'tile_col': choice[1],\n",
    "#             'info_gain': info_gain\n",
    "#         })\n",
    "\n",
    "# tile_info_df = pd.DataFrame(rows)\n",
    "# tile_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976d0f6-3757-4e7b-a9cf-d87e461f5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in tile_info_df[tile_info_df['subject_id'] == 'YA01'].iterrows():\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e7cca-9fa0-45f9-bd6f-29dc9e00a842",
   "metadata": {},
   "source": [
    "## Add infos to main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d33ecb-ea99-4cca-bcb3-9c6f89533224",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(expected_infos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8a6c2-e3c9-419f-b465-99b3af988ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exp_choices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ee25b-6e9f-482e-a4b2-cfc8701f29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add info cols to main exp df\n",
    "info_cols = [col for col in expected_infos_df.columns if col.startswith(\"info_tile_\")]\n",
    "info_cols.append(\"info_chosen\")\n",
    "print(len(info_cols))\n",
    "print(info_cols)\n",
    "\n",
    "for col in info_cols:\n",
    "    exp_choices_df[col] = expected_infos_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfa31e-d5c1-4347-a0b5-aa7dfba22781",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_choices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60729855-7010-4412-8914-db9ef28d76be",
   "metadata": {},
   "source": [
    "## Reward-weighted distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3b320-9ac5-469c-9aea-529e7f02e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only distance and reward columns, giving one df for each\n",
    "dist_cols = exp_choices_df.filter(regex=r\"^dist_tile_\\d_\\d$\")\n",
    "rew_cols = exp_choices_df.filter(regex=r\"^rew_tile_\\d_\\d$\")\n",
    "\n",
    "# extract suffixes (tile coords)\n",
    "suffixes = dist_cols.columns.str[-3:].unique()\n",
    "\n",
    "# for each tile coord, find dist column and reward column with same tile coord and multiply for weighted rew distance\n",
    "for suffix in suffixes:\n",
    "    dist_col = f\"dist_tile_{suffix}\"\n",
    "    rew_col = f\"rew_tile_{suffix}\"\n",
    "    weighted_col = f\"rw_dist_tile_{suffix}\"\n",
    "\n",
    "    if dist_col in exp_choices_df.columns and rew_col in exp_choices_df.columns:\n",
    "        exp_choices_df[weighted_col] = exp_choices_df[dist_col] * exp_choices_df[rew_col]\n",
    "    else:\n",
    "        print(f\"Missing: {dist_col} or {rew_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2d2e0-0d27-421f-bc3e-b43029d9be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_choices_df[(exp_choices_df['chosen_tile_row']==1)&(exp_choices_df['chosen_tile_col']==3)][['trial_num','choice_num','dist_tile_1_3', 'rew_tile_1_3','rw_dist_tile_1_3']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b4010-c2ce-4047-af84-58a62e718257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "print(exp_choices_df[['dist_tile_1_3', 'rew_tile_1_3','rw_dist_tile_1_3']].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894ea82-349e-4b5a-b365-a576e8417241",
   "metadata": {},
   "source": [
    "## Info-weighted distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf63fb-de07-4eb8-b9f9-80de517f3731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select only distance and info columns, giving one df for each\n",
    "dist_cols = exp_choices_df.filter(regex=r\"^dist_tile_\\d_\\d$\")\n",
    "info_cols = exp_choices_df.filter(regex=r\"^info_tile_\\d_\\d$\")\n",
    "\n",
    "# extract suffixes (tile coords)\n",
    "suffixes = dist_cols.columns.str[-3:].unique()\n",
    "\n",
    "# for each tile coord, find dist column and reward column with same tile coord and multiply for weighted rew distance\n",
    "for suffix in suffixes:\n",
    "    dist_col = f\"dist_tile_{suffix}\"\n",
    "    info_col = f\"info_tile_{suffix}\"\n",
    "    weighted_col = f\"info_dist_tile_{suffix}\"\n",
    "\n",
    "    if dist_col in exp_choices_df.columns and info_col in exp_choices_df.columns:\n",
    "        exp_choices_df[weighted_col] = exp_choices_df[dist_col] * exp_choices_df[info_col]\n",
    "    else:\n",
    "        print(f\"Missing: {dist_col} or {info_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f7430-7088-4df2-b5b6-9702f74fdf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "print(exp_choices_df[['dist_tile_1_3', 'info_tile_1_3','info_dist_tile_1_3']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab10ea-b006-4521-bf87-852ac170d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where choice_num = 1, because after trial 1, these hold irrelevant information about distance\n",
    "exp_choices_df=exp_choices_df[exp_choices_df['choice_num']!=1]\n",
    "exp_choices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31b840-79ea-4be0-bf2a-2e817d54bc0e",
   "metadata": {},
   "source": [
    "## Save `exp_choices_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec58f28-e237-4555-a85d-627e2a4f9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_choices_df.to_csv(\"exp_choices_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d0ab4-a220-4f0b-8024-b9ed665d8ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = exp_choices_df.loc[\n",
    "    exp_choices_df['subject_id'] == 'YA01',\n",
    "    ['subject_id', 'choice_num', 'trial_num', 'choice_outcome', 'rew_chosen', 'info_chosen']\n",
    "]\n",
    "\n",
    "print(filtered_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b91a3-c475-44e4-affc-80da3337ad5c",
   "metadata": {},
   "source": [
    "## Load in `exp_choices_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dc17a-9dc9-49b0-8aa4-ece87376b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_choices_df=pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/analysis/exp_choices_df.csv')\n",
    "exp_choices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9aa71-514e-43f0-9f83-4a80dbfe7a93",
   "metadata": {},
   "source": [
    "## Determining Random vs. Directed Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82e1d5cd-161f-4f8c-9ac8-ab742fd19df0",
   "metadata": {},
   "source": [
    "### About\n",
    "We want to determine how much of the exploration in participants was *directed exploration*, where they were intentionally exploring for information or reward. Actual exploration (participant behavior) is made up of random (or expected) exploration and directed exploration.\n",
    "\n",
    "#### Random/Expected Exploration\n",
    "- This is what we would expect from participants if their exploration was truly random\n",
    "- Truly random exploration; not driven by any other motivations\n",
    "\n",
    "#### Directed Exploration \n",
    "- Exploring specifically for **reward** or **information**\n",
    "\n",
    "#### Basic Formula\n",
    "##### Actual exploration = Expected/Random exploration + Directed exploration\n",
    "- **Actual exploration:** the distance participants actually went; the distance between chosen tiles\n",
    "- **Expected/Random exploration:** the expected exploration distance if truly random; equals the average of all possible distances after a move\n",
    "- **Directed exploration:** the deviation from random exploration; the difference between their actual distance and the predicted distance if exploration were random\n",
    "\n",
    "##### Thus, Directed exploration = Actual exploration - Expected/Random exploration \n",
    "- If directed exploration (i.e., the difference between their actual exploration and the expected/random exploration) is high, it means there is a large deviation from random exploration -- hence other motivators are driving the exploratory behavior\n",
    "- (Akram's words) \"If absolute difference between actual and expected is small then mostly random. If deviation from expected is high then directed\"\n",
    "\n",
    "#### Directed Info. Exploration vs. Directed Reward Exploration\n",
    "Information or reward could be driving directed exploration. We need to look at both separately. We can weight the actual and random exploration and find the weighted directed exploration that way for both directed information exploration and directed reward exploration and then the overall weighted directed exploration (by averaging the info and reward ones)\n",
    "\n",
    "##### Weighted Actual Exploration:\n",
    "- Reward-weighted actual exploration = actual distance x actual reward of chosen tile\n",
    "- Information-weighted actual exploration = actual distance x actual information of chosen tile\n",
    "\n",
    "##### Weighted Expected/Random Exploration: \n",
    "- Weighted random exploration = the average of (distance x reward of chosen tile) and (distance x information of tile)\n",
    "- Should it be the average of all tiles distances*rewards and infos?\n",
    "- Like take each tile and multiple the distance by reward/info and then average that for a weighted random reward and weighted random info then average those two?\n",
    "\n",
    "##### Thus, weighted directed exploration:\n",
    "-  **Directed info. exploration** = (actual distance x actual information of chosen tile) - (Average of all tiles distances weighted by their information)\n",
    "- **Directed reward exploration** =  (actual distance x actual reward of chosen tile) - (Average of all tiles distances weighted by their reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e96a64-2c77-4a86-948c-70c053349a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make base df to store exploration amounts for each choice for each trial for each subject\n",
    "calculated_expl_df=exp_choices_df.copy()\n",
    "calculated_expl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29d83a-80bf-4776-9cc9-32cac1c254ae",
   "metadata": {},
   "source": [
    "### Calculate exploration types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3aa41-653f-4b42-b67a-56f03d500807",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' not used but just here for conceptual basic understanding for now '''\n",
    "### simple exploration\n",
    "## simple actual exploration\n",
    "calculated_expl_df['actual_expl_simp'] = calculated_expl_df['dist_chosen']\n",
    "\n",
    "## simple random exploration\n",
    "# extract distance column names \n",
    "simp_dist_cols = calculated_expl_df.filter(regex=r\"^dist_tile_\\d+_\\d+$\").columns\n",
    "# calculate the row-wise mean of distance columns\n",
    "calculated_expl_df[\"ran_expl_simp\"] = calculated_expl_df[simp_dist_cols].mean(axis=1)\n",
    "\n",
    "## simple directed exploration\n",
    "calculated_expl_df[\"dir_expl_simp\"] = calculated_expl_df['actual_expl_simp']-calculated_expl_df[\"ran_expl_simp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be548e-9fcd-4ea9-b222-646c05d87f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### weighted exploration\n",
    "## weighted actualy exploration (the distance they actually went)\n",
    "# actual reward-weighted (distance of chosen tile*reward of chosen tile)\n",
    "calculated_expl_df['actual_expl_r_weighted'] = calculated_expl_df['dist_chosen']*calculated_expl_df['rew_chosen']\n",
    "# actual info-weighted (distance of chosen tile*info of chosen tile)\n",
    "calculated_expl_df['actual_expl_i_weighted'] = calculated_expl_df['dist_chosen']*calculated_expl_df['info_chosen']\n",
    "# actual combined-weighted (average of reward-weighted total exploration and info-weighted total exploration)\n",
    "# calculated_expl_df['actual_expl_weighted_comb'] = calculated_expl_df[['actual_expl_r_weighted','actual_expl_i_weighted']].mean(axis=1)\n",
    "\n",
    "## weighted random exploration (the average of all possible distances)\n",
    "# reward-weighted random (mean of all reward weighted distances)\n",
    "# extract reward weighted distance column names \n",
    "rw_dist_cols = calculated_expl_df.filter(regex=r\"^rw_dist_tile_\\d+_\\d+$\").columns\n",
    "# calculate the row-wise mean of reward-weighted distance columns\n",
    "calculated_expl_df[\"ran_expl_r_weighted\"] = calculated_expl_df[rw_dist_cols].mean(axis=1)\n",
    "\n",
    "# info-weighted random (mean of all info weighted distances)\n",
    "# extract reward weighted distance column names \n",
    "iw_dist_cols = calculated_expl_df.filter(regex=r\"^info_dist_tile_\\d+_\\d+$\").columns\n",
    "# calculate the row-wise mean of reward-weighted distance columns\n",
    "calculated_expl_df[\"ran_expl_i_weighted\"] = calculated_expl_df[iw_dist_cols].mean(axis=1)\n",
    "\n",
    "\n",
    "## weighted directed exploration (actual - random)\n",
    "# reward directed\n",
    "calculated_expl_df[\"rew_dir_expl_simp\"] = ((calculated_expl_df[\"actual_expl_r_weighted\"]-calculated_expl_df[\"ran_expl_r_weighted\"]).abs())\n",
    "\n",
    "# information-directed\n",
    "calculated_expl_df[\"info_dir_expl_simp\"] = ((calculated_expl_df[\"actual_expl_i_weighted\"]-calculated_expl_df[\"ran_expl_i_weighted\"]).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60872c5b-34c4-4095-a0ea-7d1e4433a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overall weighted directed exploration column\n",
    "calculated_expl_df['overall_dir_expl'] = calculated_expl_df[['rew_dir_expl_simp','info_dir_expl_simp']].mean(axis=1)\n",
    "calculated_expl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66487e-1cbe-41d1-9d51-65d03df7fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find overall weighted actual exploration\n",
    "calculated_expl_df['overall_actual_expl'] = calculated_expl_df[['actual_expl_r_weighted','actual_expl_i_weighted']].mean(axis=1)\n",
    "calculated_expl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132bab0-8afb-417a-9cb7-99d89e8926da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find overall weighted random exploration\n",
    "calculated_expl_df['overall_ran_expl'] = calculated_expl_df[['ran_expl_r_weighted','ran_expl_i_weighted']].mean(axis=1)\n",
    "calculated_expl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bdcfb-709f-477f-915b-e66d731c1dfe",
   "metadata": {},
   "source": [
    "### Save `calculated_expl_df` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ecd49-9f29-4bb3-b6ef-cd644f09e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_expl_df.to_csv(\"calculated_expl_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31054404-ef31-4efc-be87-b68d7bbdaadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print col names \n",
    "for col in calculated_expl_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041c58c-5916-4aa8-93cf-ba2f07b1f52f",
   "metadata": {},
   "source": [
    "### Load in `calculated_expl_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3edf73-2a79-410d-be78-45814221db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_expl_df=pd.read_csv('/Users/agshivers/Library/CloudStorage/Box-Box/Bakkour-Lab/projects/Battleship_task/analysis/calculated_expl_df.csv')\n",
    "calculated_expl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b293d5-4ec7-41ee-9207-1e494fc0b48b",
   "metadata": {},
   "source": [
    "### Refine df for easier viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4732f4-29a3-4517-82e9-5727ae4baa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of calculated_expl_df\n",
    "short_calculated_expl_df = calculated_expl_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d545be-c7b2-40c7-aad3-9d29482c7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine columns present\n",
    "short_calculated_expl_df = short_calculated_expl_df[[\n",
    "    'subject_id', \n",
    "    'trial_num', \n",
    "    'choice_num', \n",
    "    'dist_chosen',\n",
    "    'rew_chosen',\n",
    "    'info_chosen',\n",
    "    'actual_expl_simp',\n",
    "    'ran_expl_simp',\n",
    "    'dir_expl_simp', \n",
    "    'actual_expl_r_weighted',\n",
    "    'actual_expl_i_weighted',\n",
    "    'ran_expl_r_weighted',\n",
    "    'ran_expl_i_weighted',\n",
    "    'rew_dir_expl_simp',\n",
    "    'info_dir_expl_simp',\n",
    "    'overall_dir_expl'\n",
    "]]\n",
    "short_calculated_expl_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
